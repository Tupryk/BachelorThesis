\documentclass[11pt]{article}

\usepackage{sectsty}
\usepackage{graphicx}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{ Learning-based Multirotor Control Enhancements }
\author{ Eckart Cobo Briesewitz }
\date{\today}

\begin{document}
\maketitle
\pagebreak

% Optional TOC
% \tableofcontents
% \pagebreak

\section{Introduction}
The rise in popularity of quadrotors has lead to a need in new methods for predicting their interactions with aerodynamic forces. A simple physical model of a quadrotors behaviour can not perfectly model the real movement of a quadrotor. Therefore there is a need for models that can learn these inaccuracies. In previous work researchers have been able to develop models for this very purpose. In the previous thesis from the IMRC Lab, a bachelor student designed two types of models to solve this issue. They can be divided into two categories, these being neural networks and decision trees (or rather decision tree ensembles). In this thesis we will take the two best performing models from the previous thesis and introduce them into a crazyflie's firmware to test how much performance gain there is.

\section{The models to be tested}
As mentioned in the introduction the models can be divided into two categories. Lets start with the neural networks.
\subsection{Neural network}
\subsection{Decision tree ensemble}

\textbf{-Table comparing the models-}

\section{Uploading the models to the crazyflie}
The crazyflie has an STM32 microcontroller which does not allow for the same flexibility one would find in the python code used to train the models, therefore uploading the models into this drone is no trivial task.
\subsection{Converting the models to C code}
The crazyflie firmware is written in c code. Therefore we will write a python script to translate the .pth (Neural Network from PyTorch) and .json (Decision tree ensembles from XGBoost) into compilable c code.
\subsubsection{Neural Network}
For the neural network we can divide the c code into some utilities that will stay the same for every model like a layer propagation function and scripts that store model specific information like weights and biases. In the utils\_nn.h and utils\_nn.c scripts you will find the layer function which takes the values of some previous layer, the weights and biases of the next layer and propagates the information. This is done via for loops as with the standard STM32 we can't simply paralelise the calculation like how PyTorch or other libraries for training models would do it.

Now we can take a look at the nn.h and nn.c scripts. These have been generated by a python script which takes a .pth file and reads it's contents. They contain a structure which defines the weights and biases of a neural network. They also define a convenient function called nn\_forward which propagates a given input string through the network.

\subsubsection{Decision tree ensemble}
For the decision tree ensemble we have a similar system where the tree\_utils.h and tree\_utils.c files define a basic tree tranversal function and the generated files tree.h and tree.c are generated by the node values of the tree defined in the .json file outputed by the XGBoost library. In the tree.h and tree.c we define a tree\_forward function which calculated the mean over all the trees in the ensemble.

\subsection{Adding the models to the firmware}

\section{Testing the performance}

\section{Results}

\end{document}
